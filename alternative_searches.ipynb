{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algolia - Alternative searches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The provided dataset contains one month of anonymized aggregated searches coming from HN Search. In this case study, we want to provide alternative queries according to a user query.\n",
    "\n",
    "We propose to use a item based collaborative filtering approach in order to suggest queries related to another query.\n",
    "\n",
    "###### From logs to usable data\n",
    "First, for each document we want it to be described by a set of related queries. To do that, we will use the query and the hits (top 10 results according to the query) fields of the search json line.\n",
    "Each time a record is in the hits, we will add the query as a description of the record. We will obtain the  vector format:\n",
    "ObjectID : [q0, q1, ... qn]\n",
    "where \n",
    "    ObjectID the internal identifier of the record \n",
    "    [q0, q1, ..., qn] the set of search queries describing the record\n",
    "Finally, we will store these vectors in an Elasticsearch indice.\n",
    "\n",
    "##### Get recommendations as alternative searches\n",
    "Elasticsearch aggregations are commonly used for faceted search feature. But Significant Terms Aggregation measures the kind of statistically significant relationships to deliver meaningful recommendations. It calculates the statistical significance of terms in the current results when compared to the background corpus.\n",
    "Applied to our use case, it means that for a user query, we will retrieve all records containing this query, and then calculate the significance of the queries in the current record results and provide the most signifcant queries to the user query.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import json\n",
    "import string\n",
    "from elasticsearch import Elasticsearch, helpers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make the following assumption to keep a query as a candidate for an alternative search:\n",
    "\n",
    "the query must not be empty\n",
    "the query must be equal or more than 3 chars\n",
    "the query must provide search results (in order to not suggest queries that retrieve no result to the user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_query(data):\n",
    "    # Remove punctuation\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    query = data['query'].translate(translator)\n",
    "    if not query or len(query) < 3 or data['nb_hits'] <= 0:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def clean_query(query):\n",
    "        translator = str.maketrans('', '', string.punctuation)\n",
    "        query = data['query'].translate(translator)\n",
    "        return query.lower().strip()\n",
    "\n",
    "def levenshtein(s, t):\n",
    "        ''' From Wikipedia article; Iterative with two matrix rows. '''\n",
    "        if s == t: return 0\n",
    "        elif len(s) == 0: return len(t)\n",
    "        elif len(t) == 0: return len(s)\n",
    "        v0 = [None] * (len(t) + 1)\n",
    "        v1 = [None] * (len(t) + 1)\n",
    "        for i in range(len(v0)):\n",
    "            v0[i] = i\n",
    "        for i in range(len(s)):\n",
    "            v1[0] = i + 1\n",
    "            for j in range(len(t)):\n",
    "                cost = 0 if s[i] == t[j] else 1\n",
    "                v1[j + 1] = min(v1[j] + 1, v0[j + 1] + 1, v0[j] + cost)\n",
    "            for j in range(len(v0)):\n",
    "                v0[j] = v1[j]\n",
    "                \n",
    "        return v1[len(t)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create query vector for each record in the dataset\n",
    "ObjectID : [q0, q1, ... qn] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"workspace/algolia/hn_insights_data/\"\n",
    "files = [f for f in listdir(path) if isfile(join(path, f))]\n",
    "files.sort()\n",
    "\n",
    "doc = {}\n",
    "for f in files:\n",
    "    with open(path + f, 'r') as logs:\n",
    "         for line in logs.readlines():\n",
    "                try:\n",
    "                    # load json data\n",
    "                    data = json.loads(line)\n",
    "                    if is_valid_query(data):\n",
    "                        if 'hits' in data:\n",
    "                            for h in data['hits']:\n",
    "                                if h not in doc.keys():\n",
    "                                    doc[h] = []\n",
    "                                query = clean_query(data['query'])\n",
    "                                if not query in doc[h]:\n",
    "                                    doc[h].append(query)\n",
    "                                \n",
    "                except ValueError:\n",
    "                    ## Line is not a valid JSON, we skip this line\n",
    "                    ## print(\"JSON decode as failed for line: %s\" %line)\n",
    "                    continue\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Display some examples of a query vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14158772 : ['real estate', 'estate', 'beyond burger', 'make money', 'mcdonalds', 'est', 'real es', 'estb', 'real estrate t'] \n",
      "4638286 : ['real estate', 'estate', 'zynga', 'plum', 'real es', 'plumm', 'plummer', 'plumme', 'plummers', 'below'] \n",
      "14719749 : ['real estate', 'ipo', 'estate', 'redfin', 'files for ipo', 'real es'] \n",
      "15508956 : ['real estate', 'wework', 'estate', 'wwork', 'critics say wework is an overvalued realestate play', 'real es', 'we work', 'overvalued'] \n",
      "10728058 : ['real estate', 'estate', 'real es'] \n",
      "1646087 : ['real estate', 'estate', 'jason fried', 'real es'] \n",
      "10263075 : ['real estate', 'oakland', 'oak', 'lease', 'real es'] \n",
      "14874910 : ['real estate', 'redfin', 'real es', 'shares s', 'real estrate t', 'redfin shares surge more than 30 in 1385m real estate tech ipo'] \n",
      "12160680 : ['real estate', 'columbia', 'columbii', 'real es'] \n",
      "15750785 : ['real estate', 'real es'] \n",
      "3374434 : ['bird', 'angry', 'birh', 'bir', 'create', 'inverse', 'show hn', 'inver', 'angr', 'ang', 'inverse of'] \n"
     ]
    }
   ],
   "source": [
    "n = 0\n",
    "for d in doc:\n",
    "    print('%s : %s ' %(d, doc[d]))\n",
    "    n += 1\n",
    "    if n > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Create an elasticsearch indice with a specific mapping\n",
    "The field id corresponds to the record ObjectId and the queries field corresponds to the set of queries describing a record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acknowledged': True, 'shards_acknowledged': True, 'index': 'alternatives'}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es = Elasticsearch(['http://localhost:9200'], timeout=14400, http_compress=True, max_retries=3)\n",
    "mapping = {\n",
    "    \"settings\": {\n",
    "        \"number_of_shards\": 1,\n",
    "        \"number_of_replicas\": 0\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"docs\": {\n",
    "            \"properties\": {\n",
    "                \"queries\": {\n",
    "                    \"type\": \"keyword\"\n",
    "                },\n",
    "                \"id\": {\n",
    "                    \"type\": \"long\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "     }\n",
    "}\n",
    "\n",
    "es.indices.create(index='alternatives', ignore=400, body=mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Indexing data in the Elasticsearch indice\n",
    "We use the parallel bulk Elasticsearch api in order to speed up indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bulk index #1 - Total documents: 5000\n",
      "Bulk index #2 - Total documents: 10000\n",
      "Bulk index #3 - Total documents: 15000\n",
      "Bulk index #4 - Total documents: 20000\n",
      "Bulk index #5 - Total documents: 25000\n",
      "Bulk index #6 - Total documents: 30000\n",
      "Bulk index #7 - Total documents: 35000\n",
      "Bulk index #8 - Total documents: 40000\n",
      "Bulk index #9 - Total documents: 45000\n",
      "Bulk index #10 - Total documents: 50000\n",
      "Bulk index #11 - Total documents: 55000\n",
      "Bulk index #12 - Total documents: 60000\n",
      "Bulk index #13 - Total documents: 65000\n",
      "Bulk index #14 - Total documents: 70000\n",
      "Bulk index #15 - Total documents: 75000\n",
      "Bulk index #16 - Total documents: 80000\n",
      "Bulk index #17 - Total documents: 85000\n",
      "Bulk index #18 - Total documents: 90000\n",
      "Bulk index #19 - Total documents: 95000\n",
      "Bulk index #20 - Total documents: 100000\n",
      "Bulk index #21 - Total documents: 105000\n",
      "Bulk index #22 - Total documents: 110000\n",
      "Bulk index #23 - Total documents: 115000\n",
      "Bulk index #24 - Total documents: 120000\n",
      "Bulk index #25 - Total documents: 125000\n",
      "Bulk index #26 - Total documents: 130000\n",
      "Bulk index #27 - Total documents: 135000\n",
      "Bulk index #28 - Total documents: 140000\n",
      "Bulk index #29 - Total documents: 145000\n",
      "Bulk index #30 - Total documents: 150000\n",
      "Bulk index #31 - Total documents: 155000\n",
      "Bulk index #32 - Total documents: 160000\n",
      "Bulk index #33 - Total documents: 165000\n",
      "Bulk index #34 - Total documents: 170000\n",
      "Bulk index #35 - Total documents: 175000\n",
      "Bulk index #36 - Total documents: 180000\n",
      "Bulk index #37 - Total documents: 185000\n",
      "Bulk index #38 - Total documents: 190000\n",
      "Bulk index #39 - Total documents: 195000\n",
      "Bulk index #40 - Total documents: 200000\n",
      "Bulk index #41 - Total documents: 205000\n",
      "Bulk index #42 - Total documents: 210000\n",
      "Bulk index #43 - Total documents: 215000\n",
      "Bulk index #44 - Total documents: 220000\n",
      "Bulk index #45 - Total documents: 225000\n",
      "Bulk index #46 - Total documents: 230000\n",
      "Bulk index #47 - Total documents: 235000\n",
      "Bulk index #48 - Total documents: 240000\n",
      "Bulk index #49 - Total documents: 245000\n",
      "Bulk index #50 - Total documents: 250000\n",
      "Bulk index #51 - Total documents: 255000\n",
      "Bulk index #52 - Total documents: 260000\n",
      "Bulk index #53 - Total documents: 265000\n",
      "Bulk index #54 - Total documents: 270000\n",
      "Bulk index #55 - Total documents: 275000\n",
      "Bulk index #56 - Total documents: 280000\n",
      "Bulk index #57 - Total documents: 285000\n",
      "Bulk index #58 - Total documents: 290000\n",
      "Bulk index #59 - Total documents: 295000\n",
      "Bulk index #60 - Total documents: 300000\n",
      "Bulk index #61 - Total documents: 305000\n",
      "Bulk index #62 - Total documents: 310000\n",
      "Bulk index #63 - Total documents: 315000\n",
      "Bulk index #64 - Total documents: 320000\n",
      "Bulk index #65 - Total documents: 325000\n",
      "Bulk index #66 - Total documents: 330000\n",
      "Bulk index #67 - Total documents: 335000\n",
      "Bulk index #68 - Total documents: 340000\n",
      "Bulk index #69 - Total documents: 345000\n",
      "Bulk index #70 - Total documents: 350000\n",
      "Bulk index #71 - Total documents: 355000\n",
      "Bulk index #72 - Total documents: 360000\n",
      "Bulk index #73 - Total documents: 365000\n",
      "Bulk index #74 - Total documents: 370000\n",
      "Bulk index #75 - Total documents: 375000\n",
      "Bulk index #76 - Total documents: 380000\n",
      "Bulk index #77 - Total documents: 385000\n",
      "Bulk index #78 - Total documents: 390000\n",
      "Bulk index #79 - Total documents: 395000\n",
      "Bulk index #80 - Total documents: 400000\n",
      "Bulk index #81 - Total documents: 405000\n",
      "Bulk index #82 - Total documents: 410000\n",
      "Bulk index #83 - Total documents: 415000\n",
      "Bulk index #84 - Total documents: 420000\n",
      "Bulk index #85 - Total documents: 425000\n",
      "Bulk index #86 - Total documents: 430000\n",
      "Bulk index #87 - Total documents: 435000\n",
      "Bulk index #88 - Total documents: 440000\n",
      "Bulk index #89 - Total documents: 445000\n",
      "Bulk index #90 - Total documents: 450000\n",
      "Bulk index #91 - Total documents: 455000\n",
      "Bulk index #92 - Total documents: 460000\n",
      "Bulk index #93 - Total documents: 465000\n",
      "Bulk index #94 - Total documents: 470000\n",
      "Bulk index #95 - Total documents: 475000\n",
      "Bulk index #96 - Total documents: 480000\n",
      "Bulk index #97 - Total documents: 485000\n",
      "Bulk index #98 - Total documents: 490000\n",
      "Bulk index #99 - Total documents: 495000\n",
      "Bulk index #100 - Total documents: 500000\n",
      "Bulk index #101 - Total documents: 505000\n",
      "Bulk index #102 - Total documents: 510000\n",
      "Bulk index #103 - Total documents: 515000\n",
      "Bulk index #104 - Total documents: 520000\n",
      "Bulk index #105 - Total documents: 525000\n",
      "Bulk index #106 - Total documents: 530000\n",
      "Bulk index #107 - Total documents: 535000\n",
      "Bulk index #108 - Total documents: 540000\n",
      "Bulk index #109 - Total documents: 545000\n",
      "Bulk index #110 - Total documents: 550000\n",
      "Bulk index #111 - Total documents: 555000\n",
      "Bulk index #112 - Total documents: 560000\n",
      "Bulk index #113 - Total documents: 565000\n",
      "Bulk index #114 - Total documents: 570000\n",
      "Bulk index #115 - Total documents: 575000\n",
      "Bulk index #116 - Total documents: 580000\n",
      "Bulk index #117 - Total documents: 585000\n",
      "Bulk index #118 - Total documents: 590000\n",
      "Bulk index #119 - Total documents: 595000\n",
      "Bulk index #120 - Total documents: 600000\n",
      "Bulk index #121 - Total documents: 605000\n",
      "Bulk index #122 - Total documents: 610000\n",
      "Bulk index #123 - Total documents: 615000\n",
      "Bulk index #124 - Total documents: 620000\n",
      "Bulk index #125 - Total documents: 625000\n",
      "Bulk index #126 - Total documents: 630000\n",
      "Bulk index #127 - Total documents: 635000\n",
      "Bulk index #128 - Total documents: 640000\n",
      "Bulk index #129 - Total documents: 645000\n",
      "Bulk index #130 - Total documents: 650000\n",
      "Bulk index #131 - Total documents: 655000\n",
      "Bulk index #132 - Total documents: 660000\n",
      "Bulk index #133 - Total documents: 665000\n",
      "Bulk index #134 - Total documents: 670000\n",
      "Bulk index #135 - Total documents: 675000\n",
      "Bulk index #136 - Total documents: 680000\n",
      "Bulk index #137 - Total documents: 685000\n",
      "Bulk index #138 - Total documents: 690000\n",
      "Bulk index #139 - Total documents: 695000\n",
      "Bulk index #140 - Total documents: 700000\n",
      "Bulk index #141 - Total documents: 705000\n",
      "Bulk index #142 - Total documents: 710000\n",
      "Bulk index #143 - Total documents: 715000\n",
      "Bulk index #144 - Total documents: 720000\n",
      "Bulk index #145 - Total documents: 725000\n",
      "Bulk index #146 - Total documents: 730000\n",
      "Bulk index #147 - Total documents: 735000\n",
      "Bulk index #148 - Total documents: 740000\n",
      "Bulk index #149 - Total documents: 745000\n",
      "Bulk index #150 - Total documents: 750000\n",
      "Bulk index #151 - Total documents: 755000\n",
      "Bulk index #152 - Total documents: 760000\n",
      "Bulk index #153 - Total documents: 765000\n",
      "Bulk index #154 - Total documents: 770000\n",
      "Bulk index #155 - Total documents: 775000\n",
      "Bulk index #156 - Total documents: 780000\n",
      "Bulk index #157 - Total documents: 785000\n",
      "Bulk index #158 - Total documents: 790000\n",
      "Bulk index #159 - Total documents: 795000\n",
      "Bulk index #160 - Total documents: 800000\n"
     ]
    }
   ],
   "source": [
    "def generate_actions(docs):\n",
    "    for d in docs:\n",
    "        json_data = {}\n",
    "        json_data[\"id\"] = d\n",
    "        json_data[\"queries\"] = doc[d]\n",
    "        yield {\n",
    "            '_op_type': 'index',\n",
    "            '_index': 'alternatives',\n",
    "            '_type': 'docs',\n",
    "            '_id': d,\n",
    "            '_source': json_data\n",
    "        }\n",
    "total = 0\n",
    "bulks = 0\n",
    "for success, result in helpers.parallel_bulk(\n",
    "    client = es,actions=generate_actions(doc),\n",
    "    chunk_size=5000,\n",
    "    thread_count=4,\n",
    "    raise_on_error=False,\n",
    "    raise_on_exception=False\n",
    "):\n",
    "    total += 1\n",
    "    if total % 5000 == 0:\n",
    "        bulks += 1\n",
    "        print('Bulk index #%s - Total documents: %s' % (bulks, total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Get the recommendations\n",
    "Build the significant terms aggregation query in order to get alternative searches of a query. We are using the mutual information as significance score in order to boost common queries. The score could be change according to the use case for one which is a compromise between common and rare (JLH) or one which foster rare queries (google normalized distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_aggregation_query(query):\n",
    "    return {\n",
    "        \"query\": {\n",
    "            \"match\": {\"queries\": query}\n",
    "        },\n",
    "        \"aggs\": {\n",
    "            \"significant_queries\": {\n",
    "            \"significant_terms\": {\"field\": \"queries\", \"mutual_information\":{}, \"size\": 50}\n",
    "            }\n",
    "        },\n",
    "        \"size\": 0\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Display several examples of alternative searches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alternative searches for \"macbook\" query: \n",
      "macbook pro | 0.001053\n",
      "mac book pro | 0.000438\n",
      "mac | 0.000326\n",
      "macbook pro 2018 | 0.000324\n",
      "keyboard | 0.000323\n",
      "apple macbook | 0.000283\n",
      "apple | 0.000276\n",
      "macbook pro keyboard | 0.000263\n",
      "macbook keyboard | 0.000220\n",
      "new macbook | 0.000199\n",
      "macbook pro keybaord | 0.000178\n",
      "--------------------------------\n",
      "Alternative searches for \"python\" query: \n",
      "ruby | 0.000210\n",
      "rust | 0.000127\n",
      "java | 0.000122\n",
      "perl | 0.000115\n",
      "python type | 0.000112\n",
      "github | 0.000110\n",
      "python data | 0.000100\n",
      "javascript | 0.000095\n",
      "python web | 0.000094\n",
      "python google | 0.000093\n",
      "python machine learning | 0.000087\n",
      "--------------------------------\n",
      "Alternative searches for \"tesla\" query: \n",
      "elon musk | 0.000487\n",
      "model 3 | 0.000477\n",
      "elon | 0.000467\n",
      "musk | 0.000463\n",
      "tesla autopilot | 0.000187\n",
      "autopilot | 0.000163\n",
      "tesla model 3 | 0.000149\n",
      "tesla whistleblower | 0.000124\n",
      "spacex | 0.000121\n",
      "model m | 0.000116\n",
      "tesla sabotage | 0.000111\n",
      "--------------------------------\n",
      "Alternative searches for \"russia\" query: \n",
      "german | 0.000073\n",
      "indict | 0.000068\n",
      "trump | 0.000068\n",
      "intelligence | 0.000065\n",
      "indicht | 0.000065\n",
      "hacking | 0.000055\n",
      "indicted | 0.000055\n",
      "telegram | 0.000053\n",
      "what a russian smile means | 0.000050\n",
      "mueller | 0.000048\n",
      "world cup | 0.000044\n",
      "--------------------------------\n",
      "Alternative searches for \"trump\" query: \n",
      "china | 0.000141\n",
      "trump tariff | 0.000131\n",
      "trump kim | 0.000125\n",
      "tariff | 0.000123\n",
      "space force | 0.000122\n",
      "trade war | 0.000107\n",
      "trump space | 0.000107\n",
      "tariffs | 0.000106\n",
      "donald | 0.000101\n",
      "kim | 0.000098\n",
      "admin | 0.000081\n",
      "--------------------------------\n",
      "Alternative searches for \"matrix riot\" query: \n",
      "riot | 0.000073\n",
      "matrix | 0.000055\n",
      "--------------------------------\n",
      "Alternative searches for \"information retrieval\" query: \n",
      "introduction to information | 0.000079\n",
      "--------------------------------\n",
      "Alternative searches for \"hacker\" query: \n",
      "hacker news | 0.000457\n",
      "news | 0.000189\n",
      "hdac | 0.000176\n",
      "hackwer news | 0.000151\n",
      "hacker new | 0.000128\n",
      "ask hn | 0.000071\n",
      "show hn hacker news | 0.000069\n",
      "new | 0.000051\n",
      "show hn | 0.000051\n",
      "hacker newsletter | 0.000034\n",
      "show hack | 0.000034\n",
      "--------------------------------\n",
      "Alternative searches for \"money\" query: \n",
      "make money | 0.000308\n",
      "how to make money | 0.000117\n",
      "ask hn | 0.000105\n",
      "get money | 0.000073\n",
      "when money is | 0.000065\n",
      "money blog | 0.000064\n",
      "job | 0.000053\n",
      "make m | 0.000051\n",
      "make mone | 0.000051\n",
      "making money | 0.000050\n",
      "how reddit plans to make money through advertising | 0.000047\n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "test_queries = ['macbook', 'python', 'tesla', 'russia', 'trump', 'matrix riot', 'information retrieval', 'hacker', 'money']\n",
    "for q in test_queries:\n",
    "    print('Alternative searches for \"%s\" query: ' %q)\n",
    "    res = es.search(index='alternatives', body=build_aggregation_query(q))\n",
    "    n = 0\n",
    "    for bucket in res['aggregations']['significant_queries']['buckets']:\n",
    "        if (bucket['key']) == q or (bucket['key'].startswith(q) and len(bucket['key'].split(' ')) == len(q.split(' '))) or levenshtein(bucket['key'], q) <= 3 :\n",
    "            continue\n",
    "        print('%s | %f' %(bucket['key'], bucket['score']))\n",
    "        n += 1\n",
    "        if n > 10:\n",
    "            break\n",
    "    print(\"--------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Conclusion\n",
    "Using item based collaborative filtering approach gives good alternative searches. Moreover, with Elasticsearch and the significant terms aggregation, it is relatively easy to implement.\n",
    "However, there are several limitations to this approach. First, there is a need of a lot of data to build relevant relationships between items. Secondly, we can see with the examples in this notebook that a pre processing of the data and post processing of the results would be essential to provide relevant alternative queries. As we can notice, there is a lot of queries that have the same meaning and also queries that are not understandable for users (incomplete words, misspelled words, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
